{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b1d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0114cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\rhian\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rhian\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -U textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388da104",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13ebda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:09:20.086804Z",
     "start_time": "2022-05-01T21:09:17.979220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rhian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import io\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "st = PorterStemmer()\n",
    "from textblob import Word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7732f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:09:20.102022Z",
     "start_time": "2022-05-01T21:09:20.088806Z"
    }
   },
   "source": [
    "#Enter your twitter credentials here\n",
    "#Rhianne\n",
    "consumer_key = \"FksIiE6KS9Z2y1L3ycnIXxKhh\"\n",
    "consumer_secret = \"4FYsVQC6Uu9LsVJe7OqhhKtuyc8Sp8k3rGm6cXeGWaMC1BTRbW\"\n",
    "access_token= \"1516834013058584582-p8BftWFDGNSkfSADaNbmFDO8tSgjLA\"\n",
    "access_token_secret= \"DNsMZV8cmOtfxTTzMM4cm48us5s3NJHmdgyrqS2qNvVGN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26803a2",
   "metadata": {},
   "source": [
    "## Import Tweets using Tweepy \n",
    "\n",
    "Create list for search words, ords. \n",
    "\n",
    "Pull all tweet data obtained from search_tweets into a json file and create tempory dataframe to store data.\n",
    "https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets\n",
    "\n",
    "Search user object to also include screen_name and locatin for tweewts\n",
    "https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de05442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter credentials for project\n",
    "consumer_key = \"ATr3GPX2ECzzrQu1NWPSz1cpU\"\n",
    "consumer_secret = \"VdUHLsnmPon7RDG5Y1IwTHCndg5kLTd4yBbZXUQ7q0sLS7yXK0\"\n",
    "access_token= \"1516171293766389765-cZBHtFeMbwzupeYnOb5B142zNLHyb1\"\n",
    "access_token_secret= \"lBCtCP0vCQwkUUIJFqzSTzepsXpbieqyJnVLeLhDOvWhz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15aff43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:09:24.217974Z",
     "start_time": "2022-05-01T21:09:24.213972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Twitter authentication and the connection to Twitter Streaming API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040010bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:10:11.105254Z",
     "start_time": "2022-05-01T21:10:11.095257Z"
    }
   },
   "outputs": [],
   "source": [
    "# create list of words to search for in tweets\n",
    "#https://docs.tweepy.org/en/stable/api.html\n",
    "words = ['vegan produce', 'vegan', 'vegetarian', 'plant based', '@vegan', '@farm', '@farmer', 'meat free', 'dairy free', 'vegan farming' , 'milk alternative', 'meat alternative', 'meat replacement', 'vegan friendly']\n",
    "train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e83342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T20:52:35.128110Z",
     "start_time": "2022-05-01T20:52:21.523036Z"
    }
   },
   "outputs": [],
   "source": [
    "# search only last 7 days of tweets\n",
    "# https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets\n",
    "\n",
    "for word in words:\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=word+'-filter:retweets', lang='en', tweet_mode='extended').items(180)\n",
    "    for tweet in tweets:\n",
    "        temp = pd.DataFrame.from_dict(tweet._json, orient='index')\n",
    "        temp = temp.T\n",
    "        temp = temp[['created_at', 'id', 'full_text','lang']]\n",
    "        temp.loc[:,'user'] = tweet._json['user']['screen_name']\n",
    "        temp['User Location'] = tweet._json['user']['location']\n",
    "        train= pd.concat([train, temp], axis=0)\n",
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7d612572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'metadata', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang'])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet._json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6e9ed463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los Angeles'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet._json['user']['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46251d4",
   "metadata": {},
   "source": [
    "## Export to CSV\n",
    "\n",
    "https://developer.twitter.com/en/docs/twitter-api/rate-limits\n",
    "\n",
    "Tweepy API has limits on how many requests you can make every 15 mins. Tweepy also has a limit of 180 requests per user every 15 minutes so multiple exports are required. \n",
    "\n",
    "You also loose data every 15 mins once api times out. Export df to CSV each time you call api to avoid this and concat seperate dataframes into one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb08ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('sentiment_analysis1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ab9dee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('sentiment_analysis2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9517a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('sentiment_analysis3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4de199dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('sentiment_analysis4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('sentiment_analysis5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8378f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "df = pd.read_csv(\"sentiment_analysis1.csv\")\n",
    "df2 = pd.read_csv(\"sentiment_analysis2.csv\")\n",
    "df3 = pd.read_csv(\"sentiment_analysis3.csv\")\n",
    "df4 = pd.read_csv(\"sentiment_analysis4.csv\")\n",
    "#df5 = pd.read_csv(\"sentiment_analysis5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83e50bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine datasets using concat\n",
    "concat1 = pd.concat([df, df2], ignore_index=True)\n",
    "concat2 = pd.concat([concat1, df3], ignore_index=True)\n",
    "concat3 = pd.concat([concat2, df4], ignore_index=True)\n",
    "tweets_df = pd.concat([concat3, df5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "19343b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed May 18 14:00:45 +0000 2022</td>\n",
       "      <td>1526925763886325762</td>\n",
       "      <td>Say hello to Dug Drinks potato milk 🥔🥛 We’re t...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed May 18 11:01:46 +0000 2022</td>\n",
       "      <td>1526880721935253511</td>\n",
       "      <td>Any #journos interested in vegan wellbeing/ski...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed May 18 10:10:22 +0000 2022</td>\n",
       "      <td>1526867783493070849</td>\n",
       "      <td>1lb homemade vegan veggie burger on a sourdoug...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue May 17 19:59:28 +0000 2022</td>\n",
       "      <td>1526653650550013952</td>\n",
       "      <td>Great talk between the plant power doctor, Dr ...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue May 17 18:20:04 +0000 2022</td>\n",
       "      <td>1526628634647199746</td>\n",
       "      <td>@AldiUK A vegan version of Aldi's Irish Countr...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>Wed May 18 02:09:17 +0000 2022</td>\n",
       "      <td>1526746717143683072</td>\n",
       "      <td>Dairy free ranch is bullshit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>Wed May 18 02:01:33 +0000 2022</td>\n",
       "      <td>1526744770357362689</td>\n",
       "      <td>@HappyCow I've never been vegan, but I have be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>Wed May 18 01:56:12 +0000 2022</td>\n",
       "      <td>1526743422463246336</td>\n",
       "      <td>Gluten, soy, nut, fruit and dairy free… anyone...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>Wed May 18 01:54:04 +0000 2022</td>\n",
       "      <td>1526742886007640067</td>\n",
       "      <td>Gluten-Free Dark Chocolate Walnut Bliss Bars. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>Fri May 13 15:28:25 +0000 2022</td>\n",
       "      <td>1525135885829799941</td>\n",
       "      <td>If you are teetering on the verge of trying ve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5319 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          created_at                   id  \\\n",
       "0     Wed May 18 14:00:45 +0000 2022  1526925763886325762   \n",
       "1     Wed May 18 11:01:46 +0000 2022  1526880721935253511   \n",
       "2     Wed May 18 10:10:22 +0000 2022  1526867783493070849   \n",
       "3     Tue May 17 19:59:28 +0000 2022  1526653650550013952   \n",
       "4     Tue May 17 18:20:04 +0000 2022  1526628634647199746   \n",
       "...                              ...                  ...   \n",
       "5314  Wed May 18 02:09:17 +0000 2022  1526746717143683072   \n",
       "5315  Wed May 18 02:01:33 +0000 2022  1526744770357362689   \n",
       "5316  Wed May 18 01:56:12 +0000 2022  1526743422463246336   \n",
       "5317  Wed May 18 01:54:04 +0000 2022  1526742886007640067   \n",
       "5318  Fri May 13 15:28:25 +0000 2022  1525135885829799941   \n",
       "\n",
       "                                              full_text retweeted lang  \n",
       "0     Say hello to Dug Drinks potato milk 🥔🥛 We’re t...     False   en  \n",
       "1     Any #journos interested in vegan wellbeing/ski...     False   en  \n",
       "2     1lb homemade vegan veggie burger on a sourdoug...     False   en  \n",
       "3     Great talk between the plant power doctor, Dr ...     False   en  \n",
       "4     @AldiUK A vegan version of Aldi's Irish Countr...     False   en  \n",
       "...                                                 ...       ...  ...  \n",
       "5314                       Dairy free ranch is bullshit       NaN   en  \n",
       "5315  @HappyCow I've never been vegan, but I have be...       NaN   en  \n",
       "5316  Gluten, soy, nut, fruit and dairy free… anyone...       NaN   en  \n",
       "5317  Gluten-Free Dark Chocolate Walnut Bliss Bars. ...       NaN   en  \n",
       "5318  If you are teetering on the verge of trying ve...       NaN   en  \n",
       "\n",
       "[5319 rows x 5 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4c053748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('retweeted', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "82569a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['retweeted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d772468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@pete_ar_fryn @simoncclark456 @ecoreflections @fleroy1974 That is the mimicry of a simple natural process, not quite the same. There are no ethical issues with eating animal products produced in this country, there would be however with the wholesale replacement of entire traditional supply chains involved in the production of meat.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view some tweets\n",
    "df['full_text'][665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b5767df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns={'full_text': 'tweet', 'lang':'language'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1d32013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69e36c6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Index' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19112/759161805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Index' object is not callable"
     ]
    }
   ],
   "source": [
    "df.columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846750cd",
   "metadata": {},
   "source": [
    "## Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new feature\n",
    "train[\"word_count\"] = train[\"tweet\"].apply(lambda x : len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b02929",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"char_count\"] = train[\"tweet\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff91c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# if a stopword is present, save into this list\n",
    "train[\"stopwords\"] = train[\"tweet\"].apply(lambda x : len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a019a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special characters\n",
    "train[\"hashtag\"] = train[\"tweet\"].apply(lambda x : len([x for x in x.split() if x.startswith(\"#\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ names\n",
    "train[\"at_names\"] = train[\"tweet\"].apply(lambda x : len([x for x in x.split() if x.startswith(\"@\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03332cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7999a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5db0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0e8a5a",
   "metadata": {},
   "source": [
    "## Basic Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_tweet'] = train['full_text'].apply(lambda x : \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93c0c760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:11:18.814429Z",
     "start_time": "2022-05-01T21:11:18.782443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhian\\AppData\\Local\\Temp/ipykernel_19112/2542136566.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['clean_tweet'] = train['clean_tweet'].str.replace(\"[^\\w\\s]\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# remove special characters\n",
    "train['clean_tweet'] = train['clean_tweet'].str.replace(\"[^\\w\\s]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_tweet'] = train['clean_tweet'].str.replace(\"ð\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cab223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any stopwords\n",
    "train['clean_tweet'] = train['clean_tweet'].apply(lambda x : \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e85c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ names\n",
    "train[\"at_names\"] = train[\"tweet\"].apply(lambda x : len([x for x in x.split() if x.startswith(\"@\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify common words\n",
    "common = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10] \n",
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove common words\n",
    "common = list(common.index) \n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in common)) \n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify rare words\n",
    "rare = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:] \n",
    "rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rare words\n",
    "rare = list(rare.index) \n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare)) \n",
    "train['tweet'].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382970db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct spelling\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove suffices like “ing”, “ly”, “s”\n",
    "#train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed915b03",
   "metadata": {},
   "source": [
    "Lemmatization is a more effective option than stemming because it converts the word into its \n",
    "root word, rather than just stripping the suffices. It makes use of the vocabulary and does a \n",
    "morphological analysis to obtain the root word. Therefore, we usually prefer using \n",
    "lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) \n",
    "train['tweet'].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a41be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "425e28f8",
   "metadata": {},
   "source": [
    "## Advance Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bigrams from our tweets using the ngrams function of the textblob \n",
    "TextBlob(train['tweet'][0]).ngrams(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a1407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784e8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c72dfd26",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8130051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:11:20.328579Z",
     "start_time": "2022-05-01T21:11:19.466638Z"
    }
   },
   "outputs": [],
   "source": [
    "train['sentiment'] = train['clean_tweet'].apply(lambda x : TextBlob(x).sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"subjectivity\"] = train[\"tweet_clean\"][:5].apply(lambda x : TextBlob(x).sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491e96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3802f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7aa8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b09dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8948d59a",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f1e29426",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19112/3574181483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxxxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xxxx' is not defined"
     ]
    }
   ],
   "source": [
    "xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For search date parameters / label will be in your Dev enviorment label\n",
    "# https://developer.twitter.com/en/account/environments\n",
    "for word in words:\n",
    "    tweets = tweepy.Cursor(api.search_full_archive, label='CA2Agriculture', query=word,\n",
    "                       fromDate='20070101000', toDate='20200101000').items(1000)\n",
    "    for tweet in tweets:\n",
    "        temp = pd.DataFrame.from_dict(tweet._json, orient='index')\n",
    "        temp = temp.T\n",
    "        temp = temp[['created_at', 'id', 'text','geo', 'coordinates','retweeted','lang']]\n",
    "        train= pd.concat([train, temp], axis=0)\n",
    "train.reset_index(inplace=True, drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
